# ğŸ“„ DocuMind - RAG-Powered Document Intelligence System

A production-ready Retrieval-Augmented Generation (RAG) system that enables intelligent querying of PDF documents using semantic search and large language models.

![Python](https://img.shields.io/badge/Python-3.11-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-1.29-red)
![ChromaDB](https://img.shields.io/badge/ChromaDB-0.4.22-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸ¯ Project Overview

DocuMind transforms how users interact with document collections by implementing a sophisticated RAG pipeline that combines semantic search with contextual AI responses. Unlike traditional keyword search, this system understands the meaning and context of queries to deliver accurate, source-cited answers.

## âœ¨ Key Features

### Core Functionality
- **Semantic Document Search**: Advanced embedding-based retrieval using sentence transformers
- **RAG Pipeline**: Retrieval-Augmented Generation for grounded, accurate responses
- **Multi-Document Support**: Process and query multiple PDFs simultaneously
- **Source Attribution**: Every answer includes citations to specific document sections
- **Comparison Mode**: Demonstrates RAG effectiveness vs. non-RAG responses

### Technical Features
- **Vector Database**: ChromaDB for efficient similarity search
- **Embedding Generation**: Sentence-transformers for high-quality text embeddings
- **Chunking Strategy**: Intelligent text splitting with overlap for context preservation
- **Performance Tracking**: Real-time metrics for response time and retrieval quality
- **Export Capabilities**: Save conversations in Markdown and JSON formats

## ğŸ—ï¸ System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Documents  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document Processor         â”‚
â”‚  â€¢ Text Extraction           â”‚
â”‚  â€¢ Intelligent Chunking      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embedding Generator        â”‚
â”‚  â€¢ Sentence Transformers     â”‚
â”‚  â€¢ Batch Processing          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector Store (ChromaDB)    â”‚
â”‚  â€¢ Efficient Storage         â”‚
â”‚  â€¢ Similarity Search         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query Processing           â”‚
â”‚  â€¢ Semantic Search           â”‚
â”‚  â€¢ Context Retrieval         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Response Generation    â”‚
â”‚  â€¢ Context-Grounded Answers  â”‚
â”‚  â€¢ Source Citations          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.10+
pip (Python package manager)
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/documind.git
cd documind
```

2. **Create virtual environment**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment** (Optional - for OpenAI)
```bash
# Create .env file
echo "OPENAI_API_KEY=your_api_key_here" > .env
```

5. **Run the application**
```bash
streamlit run app.py
```

The application will open in your browser at `http://localhost:8501`

## ğŸ“– Usage Guide

### Basic Workflow

1. **Upload Documents**
   - Click "Upload PDFs" in the sidebar
   - Select one or more PDF files
   - Click "Process Documents"

2. **Query Your Documents**
   - Type your question in the chat input
   - Receive contextual answers with source citations
   - View performance metrics

3. **Compare RAG Performance**
   - Switch to "Compare RAG" mode
   - See the difference between RAG and non-RAG responses
   - Understand the value of document-grounded answers

### Advanced Features

**Performance Analytics**
- Navigate to the "Analytics" tab
- View response times, query history, and retrieval statistics

**Export Conversations**
- Go to "Settings" tab
- Export chat history as Markdown or JSON
- Include performance metrics in exports

## ğŸ› ï¸ Technical Implementation

### Document Processing Pipeline
```python
# Text extraction and chunking
DocumentProcessor()
  â”œâ”€â”€ extract_text_from_pdf()  # PyPDF2 extraction
  â”œâ”€â”€ chunk_text()              # Overlapping chunks
  â””â”€â”€ process_pdf()             # Complete pipeline
```

**Chunking Strategy:**
- Chunk Size: 1000 characters
- Overlap: 200 characters
- Preserves context across chunk boundaries

### Embedding Generation
```python
# Sentence transformer embeddings
EmbeddingGenerator()
  â”œâ”€â”€ Model: all-MiniLM-L6-v2
  â”œâ”€â”€ Dimension: 384
  â””â”€â”€ Batch processing for efficiency
```

### Vector Storage & Retrieval
```python
# ChromaDB for similarity search
VectorStore()
  â”œâ”€â”€ add_documents()     # Store embeddings
  â”œâ”€â”€ search()            # Semantic retrieval
  â””â”€â”€ get_stats()         # Collection metrics
```

**Search Strategy:**
- Cosine similarity for ranking
- Top-K retrieval (default: 3 chunks)
- Distance-based filtering

### LLM Integration
```python
# Flexible LLM support
LLMHandler()
  â”œâ”€â”€ OpenAI API (with fallback)
  â”œâ”€â”€ Local model support (Ollama)
  â””â”€â”€ Context-aware prompting
```

## ğŸ“Š Performance Metrics

The system tracks and displays:
- **Response Time**: End-to-end query processing
- **Retrieval Quality**: Number of relevant chunks
- **Source Attribution**: Documents referenced
- **Query History**: Recent queries with timestamps

## ğŸ“ Educational Value

This project demonstrates proficiency in:

**Data Science & ML**
- Natural Language Processing (NLP)
- Semantic similarity and embeddings
- Vector databases and similarity search
- Large Language Model (LLM) integration

**Software Engineering**
- Production-ready code architecture
- Error handling and logging
- Performance optimization
- User interface design

**MLOps & Deployment**
- Modular code structure
- Environment management
- Configuration handling
- Metrics tracking

## ğŸ”§ Configuration

Key settings in `src/config.py`:
```python
CHUNK_SIZE = 1000           # Characters per chunk
CHUNK_OVERLAP = 200         # Overlap between chunks
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
TOP_K_RESULTS = 3           # Chunks to retrieve
LLM_MODEL = "gpt-3.5-turbo" # OpenAI model
```

## ğŸ“ Project Structure
```
documind/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ document_processor.py  # PDF processing & chunking
â”‚   â”œâ”€â”€ embeddings.py          # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py        # ChromaDB operations
â”‚   â”œâ”€â”€ llm_handler.py         # LLM integration
â”‚   â”œâ”€â”€ comparison.py          # RAG comparison logic
â”‚   â”œâ”€â”€ metrics.py             # Performance tracking
â”‚   â””â”€â”€ export_utils.py        # Export functionality
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/               # Uploaded PDFs
â”‚   â””â”€â”€ vectordb/              # ChromaDB storage
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env                       # Environment variables
â””â”€â”€ README.md                  # This file
```

## ğŸš§ Future Enhancements

- [ ] Multi-language document support
- [ ] Advanced filtering and metadata search
- [ ] Document comparison features
- [ ] Batch query processing
- [ ] Custom embedding model fine-tuning
- [ ] REST API for programmatic access
- [ ] Docker containerization
- [ ] Cloud deployment (AWS/Azure/GCP)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- LinkedIn: [your-linkedin](https://linkedin.com/in/your-profile)
- GitHub: [your-github](https://github.com/your-username)
- Email: your.email@example.com

## ğŸ™ Acknowledgments

- **Streamlit** for the intuitive web framework
- **ChromaDB** for efficient vector storage
- **Sentence-Transformers** for embedding models
- **OpenAI** for LLM capabilities
- **LangChain** community for RAG inspiration

## ğŸ“š References

- [Retrieval-Augmented Generation (RAG) Paper](https://arxiv.org/abs/2005.11401)
- [Sentence-BERT Paper](https://arxiv.org/abs/1908.10084)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)

---

**â­ If you find this project useful, please consider giving it a star!**
